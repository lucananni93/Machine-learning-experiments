{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "This notebook is my adaptation of <a href=\"https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0\">this wonderful tutorial by Arthut Juliani</a> where I try to learn reinforcement learning.\n",
    "\n",
    "## Part 0 - Q-Learning with Tables and Neural Networks\n",
    "Q-Learning attempts to learn the value of being in a given state, and taking a specific action there.\n",
    "\n",
    "### The game: FrozenLake\n",
    "For this tutorial we are going to be attempting to solve the FrozenLake environment from the OpenAI gym.\n",
    "\n",
    "The FrozenLake environment consists of a 4x4 grid of blocks, each one either being the start block, the goal block, a safe frozen block, or a dangerous hole. The objective is to have an agent learn to navigate from the start to the goal without moving onto a hole. At any given time the agent can choose to move either up, down, left, or right. The catch is that there is a wind which occasionally blows the agent onto a space they didn’t choose. As such, perfect performance every time is impossible, but learning to avoid the holes and reach the goal are certainly still doable.\n",
    "\n",
    "The reward at every step is 0, except for entering the goal, which provides a reward of 1. Thus, we will need an algorithm that learns long-term expected rewards. This is exactly what Q-Learning is designed to provide.\n",
    "\n",
    "### Q-Learning\n",
    "In it’s simplest implementation, Q-Learning is a table of values for every state (row) and action (column) possible in the environment. Within each cell of the table, we learn a value for how good it is to take a given action within a given state.\n",
    "\n",
    "In the case of the FrozenLake environment, we have 16 possible states (one for each block), and 4 possible actions (the four directions of movement), giving us a 16x4 table of Q-values. We start by initializing the table to be uniform (all zeros), and then as we observe the rewards we obtain for various actions, we update the table accordingly.\n",
    "\n",
    "#### The update rule: Bellman equation\n",
    "We make updates to our Q-table using something called the **Bellman equation**, which states that the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state.\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma\\left(\\max Q(s', a')\\right)\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "- $s$ is the current state, while $s'$ a future state\n",
    "- $a$ is the action taken in this moment, while $a'$ is a future action\n",
    "- $r$ is the current reward\n",
    "- $\\gamma$ is a discount factor\n",
    "\n",
    "This says that the Q-value for a given state and action should represent the current reward plus the maximum discounted future reward expected according to our own table for the next state we would end up in. The discount variable allows us to decide how important the possible future rewards are compared to the present reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros((env.observation_space.n, env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95\n",
    "num_episodes = 2000\n",
    "limit_per_episode = 99\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    s = env.reset()\n",
    "    \n",
    "    while j < limit_per_episode:\n",
    "        j += 1\n",
    "        # Choose an action by picking (with noise) from Q table\n",
    "        # use less noise at each episode...more confidence\n",
    "        a = np.argmax(Q[s, :] + np.random.randn(1, env.action_space.n)/(i + 1))\n",
    "        \n",
    "        # Perform the action and get the state\n",
    "        new_s, reward, done, _ = env.step(a)\n",
    "        \n",
    "        # Update the Q table accordingly\n",
    "        Q[s, a] = reward + gamma*np.max(Q[new_s, :]\n",
    "        # if we succeded, end the episode\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TimeLimit' object has no attribute 'P'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-2fc5b7cfeb2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'TimeLimit' object has no attribute 'P'"
     ]
    }
   ],
   "source": [
    "env.compu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
