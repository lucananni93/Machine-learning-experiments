{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "logdir = \"../../logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n",
    "\n",
    "\n",
    "def load_mnist(path=\"../../data/MNIST/\"):\n",
    "    X_train = read_idx(os.path.join(path, \"X_train\"))\n",
    "    X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "                       \n",
    "    Y_train = read_idx(os.path.join(path, \"Y_train\"))\n",
    "                       \n",
    "    X_test = read_idx(os.path.join(path, \"X_test\"))\n",
    "    X_test = X_test.reshape((X_test.shape[0], -1))\n",
    "                      \n",
    "    Y_test = read_idx(os.path.join(path, \"Y_test\"))\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).repeat().batch(batch_size)\n",
    "testing_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z,reuse=None):\n",
    "    with tf.variable_scope('generator',reuse=reuse):\n",
    "        hidden1=tf.layers.dense(inputs=z,units=128,activation=tf.nn.leaky_relu)\n",
    "        hidden2=tf.layers.dense(inputs=hidden1,units=128,activation=tf.nn.leaky_relu)\n",
    "        output=tf.layers.dense(inputs=hidden2,units=784,activation=tf.nn.tanh)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "def discriminator(X,reuse=None):\n",
    "    with tf.variable_scope('discriminator',reuse=reuse):\n",
    "        hidden1=tf.layers.dense(inputs=X,units=128,activation=tf.nn.leaky_relu)\n",
    "        hidden2=tf.layers.dense(inputs=hidden1,units=128,activation=tf.nn.leaky_relu)\n",
    "        logits=tf.layers.dense(hidden2,units=1)\n",
    "        output=tf.sigmoid(logits)\n",
    "        \n",
    "        return output,logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_size = 100\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "real_images=tf.placeholder(tf.float32,shape=[None,X_train.shape[1]])\n",
    "z=tf.placeholder(tf.float32,shape=[None,Z_size])\n",
    "\n",
    "G=generator(z)\n",
    "D_output_real,D_logits_real=discriminator(real_images)\n",
    "D_output_fake,D_logits_fake=discriminator(G,reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(logits_in,labels_in):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_in,labels=labels_in))\n",
    "\n",
    "D_real_loss=loss_func(D_logits_real,tf.ones_like(D_logits_real)*0.9) #Smoothing for generalization\n",
    "D_fake_loss=loss_func(D_logits_fake,tf.zeros_like(D_logits_real))\n",
    "D_loss=D_real_loss+D_fake_loss\n",
    "\n",
    "G_loss= loss_func(D_logits_fake,tf.ones_like(D_logits_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "\n",
    "#Do this when multiple networks interact with each other\n",
    "tvars=tf.trainable_variables()  #returns all variables created(the two variable scopes) and makes trainable true\n",
    "d_vars=[var for var in tvars if 'discriminator' in var.name]\n",
    "g_vars=[var for var in tvars if 'generator' in var.name]\n",
    "\n",
    "D_trainer=tf.train.AdamOptimizer(lr).minimize(D_loss,var_list=d_vars)\n",
    "G_trainer=tf.train.AdamOptimizer(lr).minimize(G_loss,var_list=g_vars)\n",
    "\n",
    "batch_size=100\n",
    "epochs=100\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Create a filewriter to write the model's graph to TensorBoard\n",
    "    writer = tf.summary.FileWriter(logdir + '/linear_regression', sess.graph)\n",
    "    \n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        num_batches=X_train.shape[0]//batch_size\n",
    "        for i in range(num_batches):\n",
    "            batch=mnist.train.next_batch(batch_size)\n",
    "            batch_images=batch[0].reshape((batch_size,784))\n",
    "            batch_images=batch_images*2-1\n",
    "            batch_z=np.random.uniform(-1,1,size=(batch_size,100))\n",
    "            _=sess.run(D_trainer,feed_dict={real_images:batch_images,z:batch_z})\n",
    "            _=sess.run(G_trainer,feed_dict={z:batch_z})\n",
    "            \n",
    "        print(\"on epoch{}\".format(epoch))\n",
    "        \n",
    "        sample_z=np.random.uniform(-1,1,size=(1,100))\n",
    "        gen_sample=sess.run(generator(z,reuse=True),feed_dict={z:sample_z})\n",
    "        \n",
    "        samples.append(gen_sample)\n",
    "\n",
    "plt.imshow(samples[0].reshape(28,28))\n",
    "plt.imshow(samples[99].reshape(28,28))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_experiments)",
   "language": "python",
   "name": "ml_experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
